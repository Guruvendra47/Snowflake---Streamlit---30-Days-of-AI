# ğŸ—“ï¸ Day 11 â€” Displaying Chat History

## ğŸ¯ Goal of the Day

For today's challenge, our goal is to **enhance our chatbot with better history management**.

We will:

* ğŸ‘‹ Add a **welcome message**
* ğŸ“Š Display **conversation statistics** in the sidebar
* ğŸ§¹ Provide a way to **clear chat history**
* ğŸ§  Pass the **full conversation context** to the LLM

Once completed, weâ€™ll have a **more polished chatbot experience** with visible conversation tracking and true memory.

> **Note:** We also use `st.rerun()` to ensure sidebar stats update immediately after each response.

---

## ğŸ§© See the Code

```python
import streamlit as st
import json
from snowflake.snowpark.functions import ai_complete

# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
    session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()

def call_llm(prompt_text: str) -> str:
    """Call Snowflake Cortex LLM."""
    df = session.range(1).select(
        ai_complete(model="claude-3-5-sonnet", prompt=prompt_text).alias("response")
    )
    response_raw = df.collect()[0][0]
    response_json = json.loads(response_raw)
    if isinstance(response_json, dict):
        return response_json.get("choices", [{}])[0].get("messages", "")
    return str(response_json)

st.title(":material/chat: Chatbot with History")

# Initialize messages
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hello! I'm your AI assistant. How can I help you today?"}
    ]

# Sidebar to show conversation stats
with st.sidebar:
    st.header("Conversation Stats")
    user_msgs = len([m for m in st.session_state.messages if m["role"] == "user"])
    assistant_msgs = len([m for m in st.session_state.messages if m["role"] == "assistant"])
    st.metric("Your Messages", user_msgs)
    st.metric("AI Responses", assistant_msgs)
    
    if st.button("Clear History"):
        st.session_state.messages = [
            {"role": "assistant", "content": "Hello! I'm your AI assistant. How can I help you today?"}
        ]
        st.rerun()

# Display all messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("Type your message..."):
    # Add and display user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate and display assistant response
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # Build the full conversation history for context
            conversation = "\n\n".join([
                f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
                for msg in st.session_state.messages
            ])
            full_prompt = f"{conversation}\n\nAssistant:"
            
            response = call_llm(full_prompt)
        st.markdown(response)
    
    # Add assistant response to state
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.rerun()

st.divider()
st.caption("Day 11: Displaying Chat History | 30 Days of AI")
```

---

## ğŸ“˜ Explanation

## ğŸ§  The Problem from Day 10

In **Day 10**, messages were stored in **Streamlit session state**, so they appeared correctly in the UI.

âŒ **But the LLM never saw that history.**

### Example Conversation â€” *Day 10 Behavior*

```
User: What's the capital of France?
AI: Paris

User: What's the population?
AI: What location are you asking about? âŒ
```

### ğŸš¨ Why this is BAD

* The chat UI **looks** like it remembers
* The LLM **actually has zero memory**
* This creates a **fake chat experience**

### âŒ Root Cause

Only the **current prompt** was sent to the LLM:

```python
response = call_llm(prompt)  # Only sends current message!
```

> âš ï¸ **Brutal truth**: Storing messages in session state alone does NOTHING for LLM memory.

---

## âœ… The Solution (Day 11)

In **Day 11**, we fix this properly by sending the **entire conversation history** to the LLM.

### Same Conversation â€” *Day 11 Behavior*

```
User: What's the capital of France?
AI: Paris

User: What's the population?
AI: Paris has approximately 2.1 million people in the city proper... âœ…
```

ğŸ¯ **Now the AI remembers context and answers follow-ups correctly.**

---

## ğŸ”‘ How It Works (High Level)

We **rebuild the entire conversation** and send it as one prompt:

```python
# Build the full conversation history for context
conversation = "\n\n".join([
    f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
    for msg in st.session_state.messages
])
full_prompt = f"{conversation}\n\nAssistant:"

response = call_llm(full_prompt)  # Sends entire conversation!
```

> ğŸ’¡ **Interview gold**: LLMs are stateless. Memory must be injected every request.



## ğŸ§© How It Works: Step-by-Step



### 1ï¸âƒ£ Initialize with a Welcome Message

```python
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hello! I'm your AI assistant. How can I help you today?"}
    ]
```

### Why this matters

* ğŸ‘‹ **Pre-populated list**: Chat doesnâ€™t start empty
* ğŸ¤ **Better UX**: Feels welcoming and alive
* ğŸ§  **Consistent baseline** for reset

---

### 2ï¸âƒ£ Sidebar Statistics and Controls

```python
with st.sidebar:
    st.header("Conversation Stats")
    user_msgs = len([m for m in st.session_state.messages if m["role"] == "user"])
    assistant_msgs = len([m for m in st.session_state.messages if m["role"] == "assistant"])
    st.metric("Your Messages", user_msgs)
    st.metric("AI Responses", assistant_msgs)
```

### Key concepts

* ğŸ§© **List comprehension** filters messages by role
* ğŸ§® `st.metric()` displays live counters

> ğŸ“Œ **Brutal mentor note**: This is state-derived UI â€” always recomputed, never stored.

---

### 3ï¸âƒ£ Clear History Button

```python
if st.button("Clear History"):
    st.session_state.messages = [
        {"role": "assistant", "content": "Hello! I'm your AI assistant. How can I help you today?"}
    ]
    st.rerun()
```

### Why this matters

* ğŸ”„ **Resets to known-good state**
* ğŸ§¹ Prevents stale context
* âš¡ `st.rerun()` refreshes UI immediately

---

### 4ï¸âƒ£ Enhanced Message Display

```python
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
```

### Why `st.markdown()`

* âœ¨ Supports rich formatting
* ğŸ“‹ Bullet points, **bold**, *italics*
* Better than `st.write()` for chat

---

### 5ï¸âƒ£ Loading Indicator with `st.spinner`

```python
with st.spinner("Thinking..."):
    response = call_llm(full_prompt)
```

### Why this matters

* â³ Shows AI is working
* ğŸ§  Prevents â€œfrozen appâ€ confusion
* ğŸ“Œ Removed in Day 12 when streaming is added

---

### 6ï¸âƒ£ Passing Conversation History to the LLM (CRITICAL)

```python
conversation = "\n\n".join([
    f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
    for msg in st.session_state.messages
])
full_prompt = f"{conversation}\n\nAssistant:"

response = call_llm(full_prompt)
```

### Why this matters

* ğŸ§  **This is the memory**
* Without it â†’ AI forgets everything
* With it â†’ Follow-up questions work

> ğŸ”¥ **Brutal truth**: Session state â‰  memory. Prompt = memory.

---

### 7ï¸âƒ£ Updating Sidebar Stats with `st.rerun()`

```python
st.session_state.messages.append({"role": "assistant", "content": response})
st.rerun()
```

### Why this matters

* ğŸ“Š Sidebar updates instantly
* âŒ Without it â†’ metrics lag one step
* âœ… Better UX and consistency

---

## ğŸ¯ Final Result

When this code runs, you get:

* ğŸ‘‹ A welcoming initial message
* ğŸ“Š Live-updating conversation stats
* ğŸ§¹ Clear History reset
* ğŸ§  **TRUE conversation memory**
* ğŸ’¬ Natural follow-up question handling

---

## ğŸ“š Resources

* ğŸ“˜ `st.metric` Documentation
* ğŸ“˜ `st.rerun` Documentation
* ğŸ§  Streamlit Session State Management




