# ğŸ—“ï¸ Day 4 â€” Caching your App

## ğŸ§  Goal of the Day

For today's challenge, our goal is to create a **Streamlit web application** that calls a **Snowflake Cortex Large Language Model (LLM)**.

We need to build an interface where a user can:

* âœï¸ Enter a prompt
* ğŸš€ Send it to a powerful AI model (like **Claude 3.5 Sonnet**) running securely inside **Snowflake**
* ğŸ“© Get a response back
* â±ï¸ See how long the request took

Once that's done, we will display the AI's answer directly in the web app, along with the execution time.

---

## ğŸ§© See the Code

```python
import streamlit as st
import time
import json
from snowflake.snowpark.functions import ai_complete

st.title(":material/cached: Caching your App")
# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
    session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()

@st.cache_data
def call_cortex_llm(prompt_text):
    model = "claude-3-5-sonnet"
    df = session.range(1).select(
        ai_complete(model=model, prompt=prompt_text).alias("response")
    )
    
    # Get and parse response
    response_raw = df.collect()[0][0]
    response_json = json.loads(response_raw)
    return response_json

prompt = st.text_input("Enter your prompt", "Why is the sky blue?")

if st.button("Submit"):
    start_time = time.time()
    response = call_cortex_llm(prompt)
    end_time = time.time()
    
    st.success(f"*Call took {end_time - start_time:.2f} seconds*")
    st.write(response)

# Footer
st.divider()
st.caption("Day 4: Caching your App | 30 Days of AI")
```

---

## ğŸ“˜ Explanation

### ğŸ” How It Works: Stepâ€‘byâ€‘Step

Let's break down what each part of the code does.

---

## 1ï¸âƒ£ Setup: Imports and Session

```python
import streamlit as st
import time
import json
from snowflake.snowpark.functions import ai_complete

# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()
```

* ğŸ“¦ `import ...`: These lines import all the necessary libraries:

  * **streamlit** â†’ web app UI
  * **time** â†’ measure response speed
  * **json** â†’ parse the LLM's output
  * **snowpark** â†’ connect to Snowflake and use AI functions

* ğŸ” **try / except block**: Automatically detects the environment and connects appropriately, working in all deployment scenarios

---

## 2ï¸âƒ£ Defining the Cortex LLM Call

```python
@st.cache_data
def call_cortex_llm(prompt_text):
    model = "claude-3-5-sonnet"
    df = session.range(1).select(
        ai_complete(model=model, prompt=prompt_text).alias("response")
    )
    
    # Get and parse response
    response_raw = df.collect()[0][0]
    response_json = json.loads(response_raw)
    return response_json
```

* ğŸ§  `@st.cache_data`: This is a Streamlit **decorator** that cleverly saves the result of this function.

  * First time â†’ calls the LLM (3â€“5 seconds)
  * Same prompt again â†’ instant response (< 0.1 sec)
  * Change even **one character** â†’ cache miss â†’ LLM runs again

* ğŸ¤– `ai_complete(...)`: Core Snowpark function that securely calls the specified model (**Claude 3.5 Sonnet**) running in **Snowflake Cortex**

* ğŸ§¾ `df.collect()[0][0]`:

  * Executes the query
  * Fetches one row and one column
  * Returns a raw text string

* ğŸ”“ `json.loads(response_raw)`:

  * Parses the raw JSON string
  * Converts it into a structured Python dictionary

---

## 3ï¸âƒ£ Building the Web App Interface

```python
prompt = st.text_input("Enter your prompt", "Why is the sky blue?")

if st.button("Submit"):
    start_time = time.time()
    response = call_cortex_llm(prompt)
    end_time = time.time()
    
    st.success(f"*Call took {end_time - start_time:.2f} seconds*")
    st.write(response)
```

* âœï¸ `st.text_input(...)`: Draws a text input box with a label and default text

* ğŸ”˜ `st.button("Submit")`:

  * Draws a button
  * Code runs **only when clicked**

* â±ï¸ `start_time` / `end_time`:

  * Capture execution time

* ğŸ” `call_cortex_llm(prompt)`:

  * Sends user input to Cortex LLM
  * Receives structured response

* âœ… `st.success(...)`:

  * Displays a green success box with elapsed time

* ğŸ“„ `st.write(response)`:

  * Displays the entire response dictionary

---

## ğŸ–¥ï¸ Final Result

When this code runs, you will see:

* A simple webpage
* A text box for input
* A submit button
* The LLM's response displayed below
* The time taken for the request

---

## ğŸ“š Resources

* ğŸ“˜ **st.cache_data Documentation**
* ğŸ“˜ **Caching in Streamlit**
* ğŸ“˜ **SiS Caching Limitations**


