# ğŸ“… Day 2  
## ğŸ¤– Hello, Cortex!

For today's challenge, our goal is to **run a large language model (LLM) directly within Snowflake** â„ï¸ğŸ§ .  
We need to create a simple **Streamlit interface** that:

- âœï¸ Accepts a user's prompt  
- ğŸ“¤ Sends it to a **Snowflake Cortex `AI_COMPLETE` function**  
- ğŸ“¥ Gets a response  
- ğŸ–¥ï¸ Displays the AI's generated response back to the user in the app  

---

## ğŸ§ª Code:

```python
import streamlit as st
from snowflake.snowpark.functions import ai_complete
import json

st.title(":material/smart_toy: Hello, Cortex!")

# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
    session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create() 

# Model and prompt
model = "claude-3-5-sonnet"
prompt = st.text_input("Enter your prompt:")

# Run LLM inference
if st.button("Generate Response"):
    df = session.range(1).select(
        ai_complete(model=model, prompt=prompt).alias("response")
    )
    
    # Get and display response
    response_raw = df.collect()[0][0]
    response = json.loads(response_raw)
    st.write(response)

# Footer
st.divider()
st.caption("Day 2: Hello, Cortex! | 30 Days of AI")
````

---

## ğŸ“– Explanation

### ğŸ§© How It Works: Step-by-Step

Let's break down what each part of the code does.

---

## âš™ï¸ Install prerequisite libraries

In forthcoming lessons we'll leverage **Snowflake's Cortex AI** ğŸ§ â„ï¸ and therefore please install the following prerequisite libraries:

```text
snowflake-ml-python==1.20.0
snowflake-snowpark-python==1.44.0
```

---

### ğŸ’» Locally

Save the above in `requirements.txt` and run:

```bash
pip install -r requirements.txt
```

Or you could also run:

```bash
pip install snowflake-ml-python==1.20.0 snowflake-snowpark-python==1.44.0
```

---

### â˜ï¸ Streamlit Community Cloud

Save the above in `requirements.txt` and include this in the GitHub repo of your app.

---

### â„ï¸ Streamlit in Snowflake

Click on the **Packages** drop-down ğŸ“¦ and enter the libraries name as shown.

---

## 1ï¸âƒ£ Import Libraries & Connect

```python
import streamlit as st
from snowflake.snowpark.functions import ai_complete
import json

# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()
```

* `import streamlit as st`: Imports the Streamlit library, which is used to build the web app's user interface (UI) ğŸ–¥ï¸
* `from snowflake.snowpark.functions ...`: Imports the specific Cortex AI function `ai_complete` that will run the LLM inference ğŸ¤–
* `try/except block`: Automatically detects the environment and uses the appropriate connection method:

  * **Streamlit in Snowflake (SiS)** â„ï¸: Uses `get_active_session()` for automatic authentication
  * **Locally or on Streamlit Community Cloud** â˜ï¸: Uses `Session.builder` with credentials from `.streamlit/secrets.toml`
* `session`: The established Snowflake connection, ready to execute queries and call Cortex AI functions

### â“ Why `ai_complete()`?

We use the Snowpark `ai_complete()` function here because it integrates naturally with **Snowpark DataFrames**.
This approach is ideal when you want to process data in a DataFrame pipeline or when you need the response as part of a SQL-like workflow.

âš ï¸ Trade-offs:

* Returns JSON that needs parsing
* Does not support streaming

ğŸ‘‰ In **Day 3**, we'll see the Python `Complete()` API which is simpler for direct calls and supports streaming.

---

## 2ï¸âƒ£ Set Up Model and UI

```python
# Model and prompt
model = "claude-3-5-sonnet"
prompt = st.text_input("Enter your prompt:")
```

* `model = "claude-3-5-sonnet"`: Sets a variable to specify which LLM we want to use from the models available in Snowflake Cortex ğŸ§ 
* `prompt = st.text_input(...)`: Creates a text input box in the Streamlit UI with the label **"Enter your prompt:"** âœï¸
* Whatever the user types is stored in the `prompt` variable

---

## 3ï¸âƒ£ Run Inference on Button Click

```python
# Run LLM inference
if st.button("Generate Response"):
    df = session.range(1).select(
        ai_complete(model=model, prompt=prompt).alias("response")
    )
```

* `if st.button(...)`: Creates a button in the UI ğŸ–±ï¸
* The code inside this block only runs when the user clicks **Generate Response**
* `session.range(1)`: Creates a single-row DataFrame (think of a one-cell spreadsheet ğŸ“Š)
* `.select()`: Runs our AI function and captures the output
* `.alias("response")`: Renames the output column for easier access

---

## 4ï¸âƒ£ Fetch and Display the Result

```python
# Get and display response
response_raw = df.collect()[0][0]
response = json.loads(response_raw)
st.write(response)
```

* `df.collect()`: Executes the query in Snowflake and pulls the data back into the app â„ï¸â¡ï¸ğŸ–¥ï¸
* `[0][0]`: Extracts the first row and first column
* `json.loads(response_raw)`: Converts the JSON string into a Python dictionary
* `st.write(response)`: Displays the final parsed response in the Streamlit app

---

## ğŸ“š Resources

* ğŸ”— Snowflake Cortex LLM Functions
* ğŸ”— COMPLETE Function Reference
* ğŸ”— Available LLM Models

---

```
```
