# ğŸ—“ï¸ Day 12 â€” Streaming Responses

## ğŸ¯ Goal of the Day

Building on **Day 11** (conversation history and sidebar stats), today we add **streaming responses** to create a more dynamic and responsive chat experience.

Instead of waiting for the full response, users will:

* âŒ¨ï¸ See the AIâ€™s reply appear **wordâ€‘byâ€‘word**
* âš¡ Experience faster *perceived* responses
* ğŸ’¬ Get a chat UI that feels like modern messaging apps

---

## ğŸ§© See the Code

```python
import streamlit as st
import json
from snowflake.snowpark.functions import ai_complete
import time

# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
    session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()

def call_llm(prompt_text: str) -> str:
    """Call Snowflake Cortex LLM."""
    df = session.range(1).select(
        ai_complete(model="claude-3-5-sonnet", prompt=prompt_text).alias("response")
    )
    response_raw = df.collect()[0][0]
    response_json = json.loads(response_raw)
    if isinstance(response_json, dict):
        return response_json.get("choices", [{}])[0].get("messages", "")
    return str(response_json)

st.title(":material/chat: Chatbot with Streaming")

# Initialize messages
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Hello! I'm your AI assistant. How can I help you today?"}
    ]

# Sidebar to show conversation stats
with st.sidebar:
    st.header("Conversation Stats")
    user_msgs = len([m for m in st.session_state.messages if m["role"] == "user"])
    assistant_msgs = len([m for m in st.session_state.messages if m["role"] == "assistant"])
    st.metric("Your Messages", user_msgs)
    st.metric("AI Responses", assistant_msgs)
    
    if st.button("Clear History"):
        st.session_state.messages = [
            {"role": "assistant", "content": "Hello! I'm your AI assistant. How can I help you today?"}
        ]
        st.rerun()

# Display all messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("Type your message..."):
    # Add and display user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Build the full conversation history for context
    conversation = "\n\n".join([
        f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
        for msg in st.session_state.messages
    ])
    full_prompt = f"{conversation}\n\nAssistant:"
    
    # Generate stream
    def stream_generator():
        response_text = call_llm(full_prompt)
        for word in response_text.split(" "):
            yield word + " "
            time.sleep(0.02)
    
    # Display assistant response with streaming
    with st.chat_message("assistant"):
        with st.spinner("Processing"):
            response = st.write_stream(stream_generator)
    
    # Add assistant response to state
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.rerun()  # Force rerun to update sidebar stats

st.divider()
st.caption("Day 12: Streaming Responses | 30 Days of AI")
```

---

## ğŸ“˜ Explanation

### ğŸ” How It Works: Step-by-Step

Day 12 focuses on **one key enhancement**: **streaming responses**.

---

## âœ… Whatâ€™s Kept from Previous Days

* ğŸ‘‹ Welcome message on initialization (Day 11)
* ğŸ§  Full conversation history passed to the LLM (Day 11)
* ğŸ“Š Sidebar with conversation statistics (Day 11)
* ğŸ§¹ Clear History button (Day 11)
* ğŸ’¬ Chat UI with `st.chat_message()` (Days 8â€“11)

---

## ğŸ†• Whatâ€™s New: Streaming Responses

```python
# Build the full conversation history for context
conversation = "\n\n".join([
    f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
    for msg in st.session_state.messages
])
full_prompt = f"{conversation}\n\nAssistant:"

# Generate stream
def stream_generator():
    response_text = call_llm(full_prompt)
    for word in response_text.split(" "):
        yield word + " "
        time.sleep(0.02)

# Display assistant response with streaming
with st.chat_message("assistant"):
    with st.spinner("Processing"):
        response = st.write_stream(stream_generator)

# Add assistant response to state
st.session_state.messages.append({"role": "assistant", "content": response})
st.rerun()
```

---

## ğŸ”„ What Changed from Day 11

* ğŸ“¦ Conversation building happens **before streaming**
* ğŸ§µ Custom Python **generator function** simulates streaming
* ğŸ¤– `call_llm(full_prompt)` still uses SQLâ€‘based `ai_complete()`
* ğŸ“ Response is split into words and yielded one at a time
* â±ï¸ `time.sleep(0.02)` adds a smooth typing effect
* ğŸŒ€ `st.write_stream()` renders the stream in realâ€‘time
* ğŸ” Final response text is stored in Session State
* ğŸ”„ `st.rerun()` updates sidebar stats immediately

---

## ğŸ’¡ Why This Approach Works Well

* ğŸŒ **Universal compatibility**: Works in SiS, Community Cloud, and locally
* ğŸš€ **Better UX**: Users see instant progress
* âš¡ **Perceived speed**: Streaming *feels* faster than waiting
* ğŸ’¬ **Natural conversation**: Mimics human typing
* ğŸ§© **Simple logic**: Easy to understand and customize

---

## ğŸ–¥ï¸ Final Result

When this code runs, you will have:

* ğŸ’¬ A chatbot with full conversation memory
* ğŸ“Š Liveâ€‘updating sidebar statistics
* âŒ¨ï¸ Realâ€‘time streaming responses
* âœ¨ A modern, productionâ€‘style chat experience

---

## ğŸ“š Resources

* ğŸ“˜ **Cortex Complete Streaming**
* ğŸ“˜ **st.write_stream Documentation**
* ğŸ“˜ **Build Conversational Apps**


