# ğŸ—“ï¸ Day 15 â€” Model Comparison Arena

## ğŸ¯ Goal of the Day

Day 15 wraps up **Week 2 (Chatbots)** by introducing a **practical model comparison tool**.

After building increasingly sophisticated chatbots (Days 8â€“14), we now answer a critical realâ€‘world question:

> **Which LLM model should I use?**

For todayâ€™s challenge, we build a **sideâ€‘byâ€‘side comparison arena** that:

* ğŸ§ª Runs the **same prompt** on two different models
* â±ï¸ Measures **latency (response time)**
* ğŸ”¢ Estimates **output token count**
* ğŸ’¬ Displays responses **sideâ€‘byâ€‘side** in chat format

This tool helps you make **informed tradeâ€‘offs** between speed, cost, and quality.

---

## ğŸ¤” Why This Matters Now

* âœ… Youâ€™ve finished building complete chatbots (Days 8â€“14)
* ğŸš€ Week 3 starts **RAG applications** tomorrow
* ğŸ¤– Different models behave very differently
* ğŸ’° Cost, latency, and quality all vary by model

This comparison arena gives you **dataâ€‘driven clarity** instead of guessing.

---

## ğŸ§© See the Code

```python
import streamlit as st
import time
import json
from snowflake.snowpark.functions import ai_complete

# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
    session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()

# Session state initialization
if "latest_results" not in st.session_state:
    st.session_state.latest_results = None

def run_model(model: str, prompt: str) -> dict:
    """Execute model and collect metrics."""
    start = time.time()

    # Call Cortex Complete function
    df = session.range(1).select(
        ai_complete(model=model, prompt=prompt).alias("response")
    )

    # Get response from dataframe
    rows = df.collect()
    response_raw = rows[0][0]
    response_json = json.loads(response_raw)

    # Extract text from response
    text = response_json.get("choices", [{}])[0].get("messages", "") if isinstance(response_json, dict) else str(response_json)

    latency = time.time() - start
    tokens = int(len(text.split()) * 4/3)  # Estimate tokens (1 token â‰ˆ 0.75 words)

    return {
        "latency": latency,
        "tokens": tokens,
        "response_text": text
    }

def display_metrics(results: dict, model_key: str):
    """Display metrics for a model."""
    latency_col, tokens_col = st.columns(2)

    latency_col.metric("Latency (s)", f"{results[model_key]['latency']:.1f}")
    tokens_col.metric("Tokens", results[model_key]['tokens'])

def display_response(container, results: dict, model_key: str):
    """Display chat messages in container."""
    with container:
        with st.chat_message("user"):
            st.write(results["prompt"])
        with st.chat_message("assistant"):
            st.write(results[model_key]["response_text"])

# Model selection
llm_models = [
    "llama3-8b",
    "llama3-70b",
    "mistral-7b",
    "mixtral-8x7b",
    "claude-3-5-sonnet",
    "claude-haiku-4-5",
    "openai-gpt-5",
    "openai-gpt-5-mini"
]

st.title(":material/compare: Select Models")
col_a, col_b = st.columns(2)

col_a.write("**Model A**")
model_a = col_a.selectbox("Model A", llm_models, key="model_a", label_visibility="collapsed")

col_b.write("**Model B**")
model_b = col_b.selectbox("Model B", llm_models, key="model_b", index=1, label_visibility="collapsed")

# Response containers
st.divider()
col_a, col_b = st.columns(2)
results = st.session_state.latest_results

for col, model_name, model_key in [(col_a, model_a, "model_a"), (col_b, model_b, "model_b")]:
    with col:
        st.subheader(model_name)
        container = st.container(height=400, border=True)

        if results:
            display_response(container, results, model_key)

        st.caption("Performance Metrics")
        if results:
            display_metrics(results, model_key)
        else:
            latency_col, tokens_col = st.columns(2)
            latency_col.metric("Latency (s)", "â€”")
            tokens_col.metric("Tokens", "â€”")

# Chat input and execution
st.divider()
if prompt := st.chat_input("Enter your message to compare models"):
    with st.status(f"Running {model_a}..."):
        result_a = run_model(model_a, prompt)
    with st.status(f"Running {model_b}..."):
        result_b = run_model(model_b, prompt)

    st.session_state.latest_results = {
        "prompt": prompt,
        "model_a": result_a,
        "model_b": result_b
    }
    st.rerun()

st.divider()
st.caption("Day 15: Model Comparison Arena | 30 Days of AI")
```

---

## ğŸ“˜ Explanation

## ğŸ§  How It Works: Stepâ€‘byâ€‘Step

Day 15 answers a **very real engineering question**:

> â“ *Which LLM should I use for my use case?*

Instead of guessing, we **measure**.

This day introduces:

* âš–ï¸ **Sideâ€‘byâ€‘side model comparison**
* â±ï¸ **Latency measurement**
* ğŸ“ **Token (output size) estimation**

---

## 1ï¸âƒ£ Model Execution and Metrics Collection

```python
import streamlit as st
import time
import json
from snowflake.snowpark.functions import ai_complete

# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
    session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()

def run_model(model: str, prompt: str) -> dict:
    """Execute model and collect metrics."""
    start = time.time()

    # Call Cortex Complete function
    df = session.range(1).select(
        ai_complete(model=model, prompt=prompt).alias("response")
    )

    # Get response from dataframe
    rows = df.collect()
    response_raw = rows[0][0]
    response_json = json.loads(response_raw)

    # Extract text from response
    text = response_json.get("choices", [{}])[0].get("messages", "") if isinstance(response_json, dict) else str(response_json)

    latency = time.time() - start
    tokens = int(len(text.split()) * 4/3)  # Estimate tokens (1 token â‰ˆ 0.75 words)

    return {
        "latency": latency,
        "tokens": tokens,
        "response_text": text
    }
```

### ğŸ” Explanation (Lineâ€‘byâ€‘Line Logic)

* `ai_complete`: Snowflake Cortex function used to run LLMs
* `get_active_session()`: Retrieves the Snowpark session required for Cortex
* `start = time.time()`: Baseline timestamp for latency measurement
* `session.range(1).select(...)`: Standard Snowpark pattern to call Cortex
* `df.collect()`: Executes the query and retrieves results
* `json.loads(response_raw)`: Parses Cortexâ€™s JSON response
* `text = response_json.get(...)`: Safely extracts response text
* `latency = time.time() - start`: Endâ€‘toâ€‘end execution time
* `tokens = int(len(text.split()) * 4/3)`: Token estimation

> âš ï¸ **Brutal truth**: Token counts are estimated. Exact billing tokens are providerâ€‘specific.

---

## 2ï¸âƒ£ Building the Sideâ€‘byâ€‘Side UI

```python
# Model selection
llm_models = [
    "llama3-8b",
    "llama3-70b",
    "mistral-7b",
    "mixtral-8x7b",
    "claude-3-5-sonnet",
    "claude-haiku-4-5",
    "openai-gpt-5",
    "openai-gpt-5-mini"
]

st.title(":material_compare: Select Models")
col_a, col_b = st.columns(2)

col_a.write("**Model A**")
model_a = col_a.selectbox("Model A", llm_models, key="model_a", label_visibility="collapsed")

col_b.write("**Model B**")
model_b = col_b.selectbox("Model B", llm_models, key="model_b", index=1, label_visibility="collapsed")
```

### ğŸ” Why this matters

* ğŸ§  **Multiple providers**: Llama, Mistral, Mixtral, Claude, OpenAI
* ğŸ§± **Two columns**: Clean visual comparison
* ğŸ§¹ `label_visibility="collapsed"`: Removes duplicate labels
* ğŸ”„ `index=1`: Ensures different default models

---

## 3ï¸âƒ£ Response Containers and Metrics Display

```python
st.divider()
col_a, col_b = st.columns(2)
results = st.session_state.latest_results

for col, model_name, model_key in [(col_a, model_a, "model_a"), (col_b, model_b, "model_b")]:
    with col:
        st.subheader(model_name)
        container = st.container(height=400, border=True)

        if results:
            display_response(container, results, model_key)

        st.caption("Performance Metrics")
        if results:
            display_metrics(results, model_key)
        else:
            latency_col, tokens_col = st.columns(2)
            latency_col.metric("Latency (s)", "â€”")
            tokens_col.metric("Tokens", "â€”")
```

### ğŸ” Why this matters

* ğŸ“¦ Fixedâ€‘height containers prevent UI jump
* ğŸ” Loop removes duplicated code
* ğŸ“Š Emptyâ€‘state placeholders improve UX

---

## 4ï¸âƒ£ Sequential Execution and Display

```python
st.divider()
if prompt := st.chat_input("Enter your message to compare models"):
    # Run models sequentially (Model A, then Model B)
    with st.status(f"Running {model_a}..."):
        result_a = run_model(model_a, prompt)
    with st.status(f"Running {model_b}..."):
        result_b = run_model(model_b, prompt)

    st.session_state.latest_results = {
        "prompt": prompt,
        "model_a": result_a,
        "model_b": result_b
    }
    st.rerun()
```

### ğŸ” Why sequential execution?

* ğŸ§  Simpler mental model
* ğŸ§ª Easier debugging
* âŒ No async complexity

> âš ï¸ **Brutal truth**: Parallel execution is faster â€” but harder. Learn sequential first.

---

## 5ï¸âƒ£ Display Helper Functions

```python
def display_response(container, results: dict, model_key: str):
    """Display chat messages in container."""
    with container:
        with st.chat_message("user"):
            st.write(results["prompt"])
        with st.chat_message("assistant"):
            st.write(results[model_key]["response_text"])


def display_metrics(results: dict, model_key: str):
    """Display metrics for a model."""
    latency_col, tokens_col = st.columns(2)

    latency_col.metric("Latency (s)", f"{results[model_key]['latency']:.1f}")
    tokens_col.metric("Tokens", results[model_key]['tokens'])
```

### ğŸ” Why helpers matter

* â™»ï¸ No duplicated UI logic
* ğŸ§¼ Cleaner main flow
* ğŸ“ Consistent formatting

---

## ğŸ¯ Final Result

When this app runs, you get:

* âš–ï¸ Two LLMs evaluated sideâ€‘byâ€‘side
* â±ï¸ Measured latency
* ğŸ“ Estimated output tokens
* ğŸ§  Dataâ€‘driven model selection

> ğŸ’¡ **Burn this into memory**: Model choice is a business decision â€” measure before you choose.

---

## ğŸ“š Resources

* ğŸ“˜ Snowflake Cortex `ai_complete`
* ğŸ“˜ `st.metric` Documentation
* ğŸ“˜ Streamlit Layouts

-
