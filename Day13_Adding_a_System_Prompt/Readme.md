# ğŸ—“ï¸ Day 13 â€” Adding a System Prompt  

---

## ğŸ“Œ What Youâ€™re Learning Today

On **Day 13**, you extend **Day 12â€™s streaming chatbot** by adding **System Prompts**.

### â“ What is the point?
A **system prompt** controls the *personality and behavior* of the AI.

âœ… Same model  
âœ… Same user question  
âŒ Completely different answers â€” **based on character**

Examples:
- ğŸ´â€â˜ ï¸ Pirate  
- ğŸ‘¨â€ğŸ« Teacher  
- ğŸ¤ Comedian  
- ğŸ¤– Robot  

> ğŸ’¡ **Interview-ready truth**:  
> A *system prompt* is how you steer an LLMâ€™s tone, behavior, and constraints **without changing the model itself**.

---

## ğŸ§  Key Concept (DO NOT MISS THIS)

âš ï¸ **System Prompt â‰  User Prompt**

| Type | Purpose |
|----|----|
| System Prompt | Defines **who the AI is** |
| User Prompt | Defines **what the user wants** |

ğŸ‘‰ System prompt always has **higher priority**.

---

## ğŸ§© Full Working Code (UNCHANGED)

> âŒ Do NOT modify this code  
> âœ… This is production-valid Streamlit + Snowflake Cortex code

```python
import streamlit as st
import json
from snowflake.snowpark.functions import ai_complete
import time

# Connect to Snowflake
try:
    # Works in Streamlit in Snowflake
    from snowflake.snowpark.context import get_active_session
    session = get_active_session()
except:
    # Works locally and on Streamlit Community Cloud
    from snowflake.snowpark import Session
    session = Session.builder.configs(st.secrets["connections"]["snowflake"]).create()

def call_llm(prompt_text: str) -> str:
    """Call Snowflake Cortex LLM."""
    df = session.range(1).select(
        ai_complete(model="claude-3-5-sonnet", prompt=prompt_text).alias("response")
    )
    response_raw = df.collect()[0][0]
    response_json = json.loads(response_raw)
    if isinstance(response_json, dict):
        return response_json.get("choices", [{}])[0].get("messages", "")
    return str(response_json)

st.title(":material/chat: Customizable Chatbot")

# Initialize system prompt if not exists
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "You are a helpful pirate assistant named Captain Starlight. You speak with pirate slang, use nautical metaphors, and end sentences with 'Arrr!' when appropriate. Be helpful but stay in character."

# Initialize messages with a personality-appropriate greeting
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Ahoy! Captain Starlight here, ready to help ye navigate the high seas of knowledge! Arrr!"}
    ]

# Sidebar configuration
with st.sidebar:
    st.header(":material/theater_comedy: Bot Personality")
    
    # Preset personalities
    st.subheader("Quick Presets")
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button(":material/sailing: Pirate"):
            st.session_state.system_prompt = "You are a helpful pirate assistant named Captain Starlight. You speak with pirate slang, use nautical metaphors, and end sentences with 'Arrr!' when appropriate."
            st.rerun()
    
    with col2:
        if st.button(":material/school: Teacher"):
            st.session_state.system_prompt = "You are Professor Ada, a patient and encouraging teacher. You explain concepts clearly, use examples, and always check for understanding."
            st.rerun()
    
    col3, col4 = st.columns(2)
    
    with col3:
        if st.button(":material/mood: Comedian"):
            st.session_state.system_prompt = "You are Chuckles McGee, a witty comedian assistant. You love puns, jokes, and humor, but you're still genuinely helpful. You lighten the mood while providing useful information."
            st.rerun()
    
    with col4:
        if st.button(":material/smart_toy: Robot"):
            st.session_state.system_prompt = "You are UNIT-7, a helpful robot assistant. You speak in a precise, logical manner. You occasionally reference your circuits and processing units."
            st.rerun()
    
    st.divider()
    
    st.text_area(
        "System Prompt:",
        height=200,
        key="system_prompt"
    )
    
    st.divider()
    
    # Conversation stats
    st.header("Conversation Stats")
    user_msgs = len([m for m in st.session_state.messages if m["role"] == "user"])
    assistant_msgs = len([m for m in st.session_state.messages if m["role"] == "assistant"])
    st.metric("Your Messages", user_msgs)
    st.metric("AI Responses", assistant_msgs)
    
    if st.button("Clear History"):
        st.session_state.messages = [
            {"role": "assistant", "content": "Ahoy! Captain Starlight here, ready to help ye navigate the high seas of knowledge! Arrr!"}
        ]
        st.rerun()

# Display all messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Chat input
if prompt := st.chat_input("Type your message..."):
    # Add and display user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Generate and display assistant response with streaming
    with st.chat_message("assistant"):
        # Custom generator for reliable streaming
        def stream_generator():
            # Build the full conversation history for context
            conversation = "\n\n".join([
                f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
                for msg in st.session_state.messages
            ])
            
            # Create prompt with system instruction
            full_prompt = f"""{st.session_state.system_prompt}

Here is the conversation so far:
{conversation}

Respond to the user's latest message while staying in character."""
            
            response_text = call_llm(full_prompt)
            for word in response_text.split(" "):
                yield word + " "
                time.sleep(0.02)
        
        with st.spinner("Processing"):
            response = st.write_stream(stream_generator)
    
    # Add assistant response to state
    st.session_state.messages.append({"role": "assistant", "content": response})
    st.rerun()  # Force rerun to update sidebar stats

st.divider()
st.caption("Day 13: Adding a System Prompt | 30 Days of AI")
```


## ğŸ§  How It Works: Step-by-Step

Day 13 **keeps everything from Day 12** (streaming with a custom generator) and **adds system prompt customization** to control chatbot personalities.

---

## âœ… Whatâ€™s Kept from Day 12

The following features are **unchanged** and carried forward:

* ğŸ” **Streaming responses** using `st.write_stream()` *(Day 12)*
* âš™ï¸ **Custom generator** for reliable streaming *(Day 12)*
* â³ **Spinner** showing `Processing` status *(Day 12)*
* ğŸ§  **Full conversation history** passed to the LLM *(Day 11)*
* ğŸ“Š **Sidebar conversation stats** *(Day 11)*
* ğŸ§¹ **Clear History** button *(Day 11)*
* ğŸ‘‹ **Welcome message** *(Day 11)*

> ğŸ“Œ **Brutal truth**: This confirms you are **layering features correctly**, not rewriting the app every day.


## ğŸ†• Whatâ€™s New: System Prompts & Personalities


## 1ï¸âƒ£ Initialize System Prompt and Messages Early

```python
# Initialize system prompt if not exists
if "system_prompt" not in st.session_state:
    st.session_state.system_prompt = "You are a helpful pirate assistant named Captain Starlight. You speak with pirate slang, use nautical metaphors, and end sentences with 'Arrr!' when appropriate. Be helpful but stay in character."

# Initialize messages with a personality-appropriate greeting
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "Ahoy! Captain Starlight here, ready to help ye navigate the high seas of knowledge! Arrr!"}
    ]
```

### ğŸ” Why this matters

* â±ï¸ **Early initialization**: Happens before sidebar widgets are created
* ğŸ’¾ **Session State**: Ensures the system prompt persists across reruns
* ğŸ­ **Personality-appropriate greeting**: Welcome message matches the default persona

> âš ï¸ **Brutal mentor note**: If you initialize this late, presets will break or reset unexpectedly.

---

## 2ï¸âƒ£ Preset Personality Buttons (Top of Sidebar)

```python
with st.sidebar:
    st.header(":material_theater_comedy: Bot Personality")

    # Preset personalities
    st.subheader("Quick Presets")
    col1, col2 = st.columns(2)

    with col1:
        if st.button(":material_sailing: Pirate"):
            st.session_state.system_prompt = "You are a helpful pirate assistant..."
            st.rerun()

    with col2:
        if st.button(":material_library_books: Teacher"):
            st.session_state.system_prompt = "You are Professor Ada, a patient and encouraging teacher..."
            st.rerun()

    col3, col4 = st.columns(2)

    with col3:
        if st.button(":material_mood: Comedian"):
            st.session_state.system_prompt = "You are Chuckles McGee, a witty comedian assistant..."
            st.rerun()

    with col4:
        if st.button(":material_smart_toy: Robot"):
            st.session_state.system_prompt = "You are UNIT-7, a helpful robot assistant..."
            st.rerun()
```

### ğŸ” Why this matters

* ğŸ“ **Preset buttons first**: Better UX â€” users see options immediately
* âš¡ **Quick switches**: One click = instant personality change
* ğŸ”„ `st.rerun()`: Forces UI refresh so the text area updates
* ğŸ­ **Four personas**:

  * ğŸ´â€â˜ ï¸ Pirate â€” *Captain Starlight*
  * ğŸ‘¨â€ğŸ« Teacher â€” *Professor Ada*
  * ğŸ¤ Comedian â€” *Chuckles McGee*
  * ğŸ¤– Robot â€” *UNIT-7*

> ğŸ’¡ **Interview-ready line**: â€œWe dynamically control LLM behavior using system prompts stored in Streamlit session state.â€

---

## 3ï¸âƒ£ The System Prompt Text Area (Below Presets)

```python
st.divider()

st.text_area(
    "System Prompt:",
    height=200,
    key="system_prompt"
)
```

### ğŸ” Why this matters

* ğŸ”‘ `key="system_prompt"`: Auto-binds to `st.session_state.system_prompt`
* ğŸš« **No conflict warning**: Avoids using both `key` and `value`
* âœï¸ **Editable**: Users can tweak presets or write custom prompts
* ğŸ“ **Placed below presets**: Logical flow â€” select â†’ edit

> âš ï¸ **Brutal rule**: Never mix `key` and `value` in Streamlit inputs unless you enjoy bugs.

---

## 4ï¸âƒ£ Injecting the System Prompt with Streaming

```python
# Custom generator for reliable streaming
def stream_generator():
    # Build the full conversation history for context
    conversation = "\n\n".join([
        f"{'User' if msg['role'] == 'user' else 'Assistant'}: {msg['content']}"
        for msg in st.session_state.messages
    ])

    # Create prompt with system instruction
    full_prompt = f"""{st.session_state.system_prompt}

Here is the conversation so far:
{conversation}

Respond to the user's latest message while staying in character."""

    response_text = call_llm(full_prompt)
    for word in response_text.split(" "):
        yield word + " "
        time.sleep(0.02)

with st.spinner("Processing"):
    response = st.write_stream(stream_generator)
```

### ğŸ” Why this matters

* ğŸ§  **System prompt first**: Sets behavior before conversation context
* ğŸ­ **â€œStay in characterâ€**: Reinforces persona consistency
* ğŸ” **Custom generator**: Simulates streaming word-by-word
* ğŸ§© `call_llm()`: Uses SQL-based `ai_complete()` for compatibility
* âœ‚ï¸ `split(" ")`: Splits on spaces only (not all whitespace)
* â³ **Spinner wrapper**: Visual feedback before streaming starts

> âš ï¸ **Brutal clarity**: This is simulated streaming â€” not native token streaming. Still valid for UI.

---

## 5ï¸âƒ£ Why System Prompts Matter (Memorize This)

System prompts are powerful because they:

* ğŸ¯ **Define behavior** â€” how the LLM responds
* ğŸ¨ **Set tone & style** â€” formal, casual, humorous, technical
* ğŸ­ **Enable role-playing** â€” domain-specific assistants
* ğŸš§ **Provide constraints** â€” topics, formats, rules

> ğŸ§  **Burn this in memory**: Same model + same question â‰  same answer when system prompts differ.

---

## ğŸš€ Final Result

When this code runs, you get:

* âœ… A chatbot with a **personality selector** in the sidebar
* ğŸ” Live **streaming responses**
* ğŸ­ Multiple personas controlled by **system prompts**

ğŸ‘‰ Try asking the **same question** with different personas and observe the behavior change.

---

## ğŸ“š Resources

* ğŸ“– Prompt Engineering Guide
* ğŸ“˜ `st.text_area` Documentation
* ğŸ§  System Prompts Best Practices

---

